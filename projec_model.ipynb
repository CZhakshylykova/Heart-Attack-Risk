{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure Clinical Records: Modeling & Survival Analysis\n",
    "\n",
    "**Goal:** Predict mortality and identify clinical risk factors using robust, interpretable models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "sns.set_palette(\"husl\")\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Feature-Engineered Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_failure.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Handle Imbalance\n",
    "- **Stratified split**\n",
    "- **SMOTE oversampling**\n",
    "- From EDA is know that the two classes are not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df.drop(columns=['DEATH_EVENT'])\n",
    "y = df['DEATH_EVENT']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.25, random_state=42\n",
    ")\n",
    "print(\"Class balance (train):\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# SMOTE oversampling on training set\n",
    "# How does SMOTE work? \n",
    "# SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic samples\n",
    "# for the minority class by interpolating between existing minority class samples.\n",
    "# This helps to balance the class distribution in the training set.\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Class balance after SMOTE:\")\n",
    "print(pd.Series(y_res).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Assumption Checks\n",
    "### (VIF, linearity, Cook's distance, perfect separation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X_vif = add_constant(X_res)\n",
    "vif_df = pd.DataFrame({\n",
    "    'feature': X_vif.columns,\n",
    "    'VIF': [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "})\n",
    "display(vif_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity of logit (visual check, on original training data)\n",
    "for col in ['age', 'ejection_fraction', 'serum_creatinine', 'platelets', 'time']:\n",
    "    df['bin'] = pd.qcut(df[col], 10, duplicates='drop')\n",
    "    grouped = df.groupby('bin')['DEATH_EVENT'].mean()\n",
    "    plt.figure()\n",
    "    grouped.plot(marker='o')\n",
    "    plt.title(f\"Linearity check: Mean DEATH_EVENT by {col} decile\")\n",
    "    plt.ylabel(\"Mean DEATH_EVENT\")\n",
    "    plt.xlabel(col)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "df.drop('bin', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cook's distance (on original data, not oversampled)\n",
    "import statsmodels.api as sm\n",
    "\n",
    "logit_mod = sm.Logit(y, add_constant(X)).fit(disp=0)\n",
    "influence = logit_mod.get_influence()\n",
    "cooks = influence.cooks_distance[0]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(np.arange(len(cooks)), cooks, markerfmt=\",\")\n",
    "plt.title(\"Cook's Distance for Influential Observations (Logistic Regression)\")\n",
    "plt.xlabel(\"Observation\")\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Observations with Cook's Distance > 4/n: {sum(cooks > 4/len(cooks))} (out of {len(cooks)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect separation check\n",
    "for col in X.columns:\n",
    "    print(f\"{col}:\")\n",
    "    print(df.groupby(col)['DEATH_EVENT'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning: Logistic Regression & Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression: Tune C (inverse regularization), solver\n",
    "lr_params = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_gs = GridSearchCV(lr, lr_params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "lr_gs.fit(X_res, y_res)\n",
    "print(\"Best Logistic Regression Params:\", lr_gs.best_params_)\n",
    "print(\"Best CV ROC AUC: %.3f\" % lr_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Params: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best CV ROC AUC: 0.953\n"
     ]
    }
   ],
   "source": [
    "# Random Forest: Tune n_estimators, max_depth, min_samples_split\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 4, 8, 12],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_gs = GridSearchCV(rf, rf_params, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "rf_gs.fit(X_res, y_res)\n",
    "print(\"Best Random Forest Params:\", rf_gs.best_params_)\n",
    "print(\"Best CV ROC AUC: %.3f\" % rf_gs.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation: Test Set Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimators for final prediction/evaluation\n",
    "best_lr = lr_gs.best_estimator_\n",
    "best_rf = rf_gs.best_estimator_\n",
    "\n",
    "probs_lr = best_lr.predict_proba(X_test)[:,1]\n",
    "preds_lr = best_lr.predict(X_test)\n",
    "probs_rf = best_rf.predict_proba(X_test)[:,1]\n",
    "preds_rf = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Best Logistic Regression ROC AUC: %.3f\" % roc_auc_score(y_test, probs_lr))\n",
    "print(\"Best Random Forest ROC AUC: %.3f\" % roc_auc_score(y_test, probs_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall curves\n",
    "plt.figure(figsize=(10,5))\n",
    "for name, probs in zip(['LR','RF'], [probs_lr, probs_rf]):\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    auc_val = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.2f})\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for name, probs in zip(['LR','RF'], [probs_lr, probs_rf]):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "    ap = average_precision_score(y_test, probs)\n",
    "    plt.plot(recall, precision, label=f\"{name} (AP={ap:.2f})\")\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and classification report (Random Forest)\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, preds_rf)\n",
    "ConfusionMatrixDisplay(cm, display_labels=['Survived','Died']).plot(cmap='coolwarm')\n",
    "plt.title(\"Random Forest: Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, preds_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Feature Importance\n",
    "importances = pd.Series(best_rf.feature_importances_, index=features)\n",
    "importances.sort_values().plot.barh(figsize=(8,5))\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) SHAP: Model Explainability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "    explainer = shap.TreeExplainer(best_rf)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.summary_plot(shap_values[1], X_test)\n",
    "except ImportError:\n",
    "    print(\"SHAP not installed (pip install shap to enable explainability plots).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Subgroup Survival Analysis (Kaplan-Meier by sex, diabetes, high BP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "for col in ['sex','diabetes','high_blood_pressure']:\n",
    "    for val in sorted(df[col].unique()):\n",
    "        label = f\"{col}={val}\"\n",
    "        ix = df[col]==val\n",
    "        kmf = KaplanMeierFitter()\n",
    "        kmf.fit(df.loc[ix,'time'], event_observed=df.loc[ix,'DEATH_EVENT'], label=label)\n",
    "        kmf.plot_survival_function()\n",
    "    plt.title(f\"Survival Curve by {col.title()}\")\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Survival Probability\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Clinical Insights & Next Steps\n",
    "\n",
    "- Imbalance addressed with SMOTE and class weights.\n",
    "- Hyperparameter tuning improved model ROC AUC.\n",
    "- Key predictors: age, ejection fraction, serum creatinine, comorbidity.\n",
    "- High-risk subgroups: age > 70, high serum creatinine, multiple comorbidities.\n",
    "- Next steps: external validation, deeper explainability, and more advanced models if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
